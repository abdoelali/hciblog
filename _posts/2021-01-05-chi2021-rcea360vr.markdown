---
layout: post
title:  "CHI 2021: Real-time, Continuous Emotion Annotation in 360◦ VR Videos for Collecting Precise Viewport-dependent Ground Truth Labels"
date:   2020-01-05 18:23:00 +0100
categories: [HCI, Affective Computing]
tags:
- hci
- affective computing
---

Precise emotion ground truth labels for 360◦ virtual reality (VR) video watching are essential for fine-grained predictions under varying viewing behavior. However, current annotation techniques either rely on post-stimulus discrete self-reports, or real-time, con- tinuous emotion annotations (RCEA) but only for desktop and mo- bile settings. We present RCEA for 360◦ VR videos (RCEA-360VR), where we evaluate in a controlled study (N=32) the usability of two peripheral visualization techniques: HaloLight and DotSize. We furthermore develop a method that considers head movements when fusing labels. Using physiological, behavioral, and subjective measures, we show that (1) both techniques do not increase users’ workload, sickness, nor break presence (2) our continuous valence and arousal annotations are consistent with discrete within-VR and original stimuli ratings (3) users exhibit high similarity in viewing behavior, where fused ratings perfectly align with intended labels.

<figure>
<img width="100%" src="{{site.baseurl}}/assets/imgs/0_Cover_abandon.png">
<figcaption> (a) RCEA-360VR system components. (b) HaloLight: shaded halo arc in bottom-right viewport. (c) DotSize: circle dot in bottom-right viewport. (d) A screen-shot of helper function. (e) Within-VR SAM Rating Panel. </figcaption>
</figure>

Unlike 2D video watching, if user annotations are performed under continuously changing viewports, this creates uncertainty that the annotations pertain to that specific scene at any given point in time. This necessitates methods that consider similarities in view- ing behavior. While existing techniques enable greater uniformity in viewing behavior (e.g., looping video textures under a gazed at region of interest [60]), or provide on-display guidance cues for where to look (e.g., Halo- and WedgeVR [36]), our goal was to al- low as much viewing freedom as possible without manipulating video content. In this respect, our showed how RCEA-360VR takes advantage of regularities in head movement patterns (cf., [82]) to ensure effective fused annotations (RQ2). For human-computer in- teraction and emotion computing researchers, this unlocks greater insight into the temporal nature of reported emotion across videos (cf., Sec 4.4.1) during immersive 360◦ VR experiences. Similarly, it enables building more temporally precise labels for training emo- tion recognition systems [6, 94, 114] that can perform predictions at a more fine-grained level.


<!-- <h1>Archive of posts from {{ page.date | date: "%Y" }}</h1>

<ul class="posts">
{% for post in site.posts %}
  <li>
    <span class="post-date">{{ post.date | date: "%b %-d, %Y" }}</span>
    <a class="post-link" href="{{ post.url | relative_url }}">{{ post.title }}</a>
  </li>
{% endfor %}
</ul> -->




<!-- {%- assign posts = site.posts | where:"categories","post" -%}
{%- assign groupedByYear = posts | group_by_exp:"post","post.date | date:'%Y' " -%}

{%- for yearitem in groupedByYear -%}
    {{ yearitem.name }}
{% endfor %} -->

<!--
{% for post in page.posts %}

{{ post.date | date: "%b %-d, %Y" }}

    <!-- <li>
      <span class="post-date">{{ post.date | date: "%b %-d, %Y" }}</span>
      <a class="post-link" href="{{ post.url | relative_url }}">{{ post.title }}</a>
    </li> -->
 <!-- {% endfor %} -->


<!-- This post is in my `_posts` directory. random mumblings go here. Link to [website:](/hciblog/) and [here](https://ad) -->

<!-- [jekyll-talk]: https://talk.jekyllrb.com/ -->
